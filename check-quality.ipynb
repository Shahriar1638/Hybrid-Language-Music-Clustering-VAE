{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326d4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix Windows console encoding\n",
    "# if sys.platform == 'win32':\n",
    "#     import io\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DATA_DIR = \"processed_data\"\n",
    "RESULTS_DIR = \"results\"\n",
    "REPORT_FILE = \"data_quality_report.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7f8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"Load all processed data files.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # Load numpy arrays\n",
    "    files_to_load = {\n",
    "        'features_normalized': 'features_normalized.npy',\n",
    "        'features_raw': 'features_raw.npy',\n",
    "        'labels': 'labels.npy'\n",
    "    }\n",
    "    \n",
    "    for key, filename in files_to_load.items():\n",
    "        filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            data[key] = np.load(filepath, allow_pickle=True)\n",
    "            print(f\"[OK] Loaded {filename}: shape {data[key].shape}, dtype {data[key].dtype}\")\n",
    "        else:\n",
    "            print(f\"[X] Missing {filename}\")\n",
    "            data[key] = None\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_path = os.path.join(PROCESSED_DATA_DIR, 'metadata.csv')\n",
    "    if os.path.exists(metadata_path):\n",
    "        data['metadata'] = pd.read_csv(metadata_path)\n",
    "        print(f\"[OK] Loaded metadata.csv: {len(data['metadata'])} rows\")\n",
    "    else:\n",
    "        print(f\"[X] Missing metadata.csv\")\n",
    "        data['metadata'] = None\n",
    "    \n",
    "    # Load config\n",
    "    config_path = os.path.join(PROCESSED_DATA_DIR, 'config.pkl')\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'rb') as f:\n",
    "            data['config'] = pickle.load(f)\n",
    "        print(f\"[OK] Loaded config.pkl\")\n",
    "    else:\n",
    "        data['config'] = None\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ac5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_consistency(data, report_lines):\n",
    "    \"\"\"Check if all data components are consistent.\"\"\"\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"1. DATA CONSISTENCY CHECK\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    if data['features_normalized'] is not None and data['labels'] is not None:\n",
    "        if len(data['features_normalized']) != len(data['labels']):\n",
    "            issues.append(f\"[ERROR] Features ({len(data['features_normalized'])}) and labels ({len(data['labels'])}) have different lengths!\")\n",
    "        else:\n",
    "            report_lines.append(f\"[OK] Features and labels have same length: {len(data['features_normalized'])} samples\")\n",
    "    \n",
    "    if data['metadata'] is not None and data['features_normalized'] is not None:\n",
    "        if len(data['metadata']) != len(data['features_normalized']):\n",
    "            issues.append(f\"[ERROR] Metadata ({len(data['metadata'])}) and features ({len(data['features_normalized'])}) have different lengths!\")\n",
    "        else:\n",
    "            report_lines.append(f\"[OK] Metadata and features have same length\")\n",
    "    \n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            report_lines.append(issue)\n",
    "    else:\n",
    "        report_lines.append(\"[OK] All data components are consistent!\")\n",
    "    \n",
    "    return len(issues) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c356054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(data, report_lines):\n",
    "    \"\"\"Analyze feature statistics and quality.\"\"\"\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"2. FEATURE ANALYSIS\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    features = data['features_normalized']\n",
    "    if features is None:\n",
    "        report_lines.append(\"[ERROR] No features to analyze\")\n",
    "        return\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    report_lines.append(f\"\\nDataset Shape: {n_samples} samples x {n_features} features\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    report_lines.append(\"\\n--- Basic Statistics ---\")\n",
    "    report_lines.append(f\"Mean: {np.mean(features):.6f}\")\n",
    "    report_lines.append(f\"Std: {np.std(features):.6f}\")\n",
    "    report_lines.append(f\"Min: {np.min(features):.6f}\")\n",
    "    report_lines.append(f\"Max: {np.max(features):.6f}\")\n",
    "    \n",
    "    # Check for NaN/Inf values\n",
    "    report_lines.append(\"\\n--- Missing/Invalid Values ---\")\n",
    "    nan_count = np.isnan(features).sum()\n",
    "    inf_count = np.isinf(features).sum()\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        report_lines.append(f\"[ERROR] Found {nan_count} NaN values ({100*nan_count/features.size:.2f}%)\")\n",
    "    else:\n",
    "        report_lines.append(\"[OK] No NaN values\")\n",
    "    \n",
    "    if inf_count > 0:\n",
    "        report_lines.append(f\"[ERROR] Found {inf_count} Inf values ({100*inf_count/features.size:.2f}%)\")\n",
    "    else:\n",
    "        report_lines.append(\"[OK] No Inf values\")\n",
    "    \n",
    "    # Check normalization quality\n",
    "    report_lines.append(\"\\n--- Normalization Quality ---\")\n",
    "    feature_means = np.mean(features, axis=0)\n",
    "    feature_stds = np.std(features, axis=0)\n",
    "    \n",
    "    near_zero_mean = np.sum(np.abs(feature_means) < 0.01)\n",
    "    near_unit_std = np.sum(np.abs(feature_stds - 1.0) < 0.1)\n",
    "    \n",
    "    report_lines.append(f\"Features with mean ~ 0: {near_zero_mean}/{n_features} ({100*near_zero_mean/n_features:.1f}%)\")\n",
    "    report_lines.append(f\"Features with std ~ 1: {near_unit_std}/{n_features} ({100*near_unit_std/n_features:.1f}%)\")\n",
    "    \n",
    "    if near_zero_mean < n_features * 0.8:\n",
    "        report_lines.append(\"[WARN] Warning: Normalization may not be optimal. Consider re-normalizing.\")\n",
    "    \n",
    "    # Check for constant features\n",
    "    constant_features = np.sum(feature_stds < 1e-6)\n",
    "    if constant_features > 0:\n",
    "        report_lines.append(f\"[WARN] Warning: {constant_features} features have near-zero variance (constant)\")\n",
    "    \n",
    "    # Check for highly correlated features\n",
    "    report_lines.append(\"\\n--- Feature Correlations ---\")\n",
    "    if n_features < 500:  # Only compute for smaller feature sets\n",
    "        corr_matrix = np.corrcoef(features.T)\n",
    "        high_corr_pairs = np.sum(np.abs(corr_matrix) > 0.95) - n_features  # Exclude diagonal\n",
    "        report_lines.append(f\"Highly correlated feature pairs (|r| > 0.95): {high_corr_pairs // 2}\")\n",
    "        if high_corr_pairs > n_features:\n",
    "            report_lines.append(\"[WARN] Warning: Many highly correlated features. Consider PCA dimensionality reduction.\")\n",
    "    else:\n",
    "        report_lines.append(\"(Skipped for large feature set)\")\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb9803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_labels(data, report_lines):\n",
    "    \"\"\"Analyze label distribution.\"\"\"\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"3. LABEL DISTRIBUTION ANALYSIS\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    labels = data['labels']\n",
    "    if labels is None:\n",
    "        report_lines.append(\"[ERROR] No labels to analyze\")\n",
    "        return\n",
    "    \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    report_lines.append(f\"\\nNumber of classes: {n_classes}\")\n",
    "    report_lines.append(f\"Total samples: {len(labels)}\")\n",
    "    \n",
    "    report_lines.append(\"\\n--- Class Distribution ---\")\n",
    "    for label, count in sorted(zip(unique_labels, counts), key=lambda x: -x[1]):\n",
    "        percentage = 100 * count / len(labels)\n",
    "        bar = \"#\" * int(percentage / 2)\n",
    "        report_lines.append(f\"{label:15s}: {count:5d} ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    report_lines.append(\"\\n--- Class Balance Metrics ---\")\n",
    "    imbalance_ratio = max(counts) / min(counts)\n",
    "    report_lines.append(f\"Imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        report_lines.append(\"[WARN] Warning: High class imbalance detected!\")\n",
    "        report_lines.append(\"   Suggestions:\")\n",
    "        report_lines.append(\"   - Consider oversampling minority classes\")\n",
    "        report_lines.append(\"   - Use class weights in training\")\n",
    "        report_lines.append(\"   - Try SMOTE or data augmentation\")\n",
    "    elif imbalance_ratio > 2:\n",
    "        report_lines.append(\"[WARN] Moderate class imbalance. Consider using class weights.\")\n",
    "    else:\n",
    "        report_lines.append(\"[OK] Class distribution is reasonably balanced\")\n",
    "    \n",
    "    return unique_labels, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28be7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(data, report_lines):\n",
    "    \"\"\"Suggest hyperparameters based on data characteristics.\"\"\"\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"4. HYPERPARAMETER RECOMMENDATIONS\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    features = data['features_normalized']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    if features is None:\n",
    "        return\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    n_classes = len(np.unique(labels)) if labels is not None else 12\n",
    "    \n",
    "    report_lines.append(\"\\n--- VAE Architecture Suggestions ---\")\n",
    "    \n",
    "    # Input dimension\n",
    "    report_lines.append(f\"\\nInput dimension: {n_features}\")\n",
    "    \n",
    "    # Latent dimension suggestions\n",
    "    if n_features > 200:\n",
    "        suggested_latent = [32, 64, 128]\n",
    "    elif n_features > 100:\n",
    "        suggested_latent = [16, 32, 64]\n",
    "    else:\n",
    "        suggested_latent = [8, 16, 32]\n",
    "    report_lines.append(f\"Suggested latent dimensions: {suggested_latent}\")\n",
    "    \n",
    "    # Encoder architecture based on feature count\n",
    "    if n_features > 200:\n",
    "        report_lines.append(f\"Suggested encoder layers: [{n_features}, 512, 256, 128]\")\n",
    "        report_lines.append(f\"Suggested decoder layers: [128, 256, 512, {n_features}]\")\n",
    "    elif n_features > 100:\n",
    "        report_lines.append(f\"Suggested encoder layers: [{n_features}, 256, 128]\")\n",
    "        report_lines.append(f\"Suggested decoder layers: [128, 256, {n_features}]\")\n",
    "    else:\n",
    "        report_lines.append(f\"Suggested encoder layers: [{n_features}, 128, 64]\")\n",
    "        report_lines.append(f\"Suggested decoder layers: [64, 128, {n_features}]\")\n",
    "    \n",
    "    # Training suggestions\n",
    "    report_lines.append(\"\\n--- Training Suggestions ---\")\n",
    "    \n",
    "    if n_samples < 1000:\n",
    "        suggested_batch = 16\n",
    "        suggested_epochs = 200\n",
    "    elif n_samples < 5000:\n",
    "        suggested_batch = 32\n",
    "        suggested_epochs = 150\n",
    "    else:\n",
    "        suggested_batch = 64\n",
    "        suggested_epochs = 100\n",
    "    \n",
    "    report_lines.append(f\"Dataset size: {n_samples} samples\")\n",
    "    report_lines.append(f\"Suggested batch size: {suggested_batch}\")\n",
    "    report_lines.append(f\"Suggested epochs: {suggested_epochs}\")\n",
    "    report_lines.append(f\"Suggested learning rate: 1e-3 to 1e-4\")\n",
    "    report_lines.append(f\"Suggested beta (KL weight): 0.1 to 1.0 (start lower)\")\n",
    "    \n",
    "    # Clustering suggestions\n",
    "    report_lines.append(\"\\n--- Clustering Suggestions ---\")\n",
    "    report_lines.append(f\"Number of known classes: {n_classes}\")\n",
    "    report_lines.append(f\"Suggested K for K-Means: {n_classes}\")\n",
    "    report_lines.append(f\"Alternative K values to try: {max(2, n_classes-2)} to {n_classes+2}\")\n",
    "    \n",
    "    # Data-specific recommendations\n",
    "    report_lines.append(\"\\n--- Data-Specific Recommendations ---\")\n",
    "    \n",
    "    # Check if data might need more preprocessing\n",
    "    feature_stds = np.std(features, axis=0)\n",
    "    if np.any(feature_stds < 0.1):\n",
    "        report_lines.append(\"[WARN] Some features have low variance. Consider:\")\n",
    "        report_lines.append(\"   - Removing low-variance features\")\n",
    "        report_lines.append(\"   - Feature selection techniques\")\n",
    "    \n",
    "    # Sample size recommendations\n",
    "    samples_per_class = n_samples / n_classes\n",
    "    if samples_per_class < 50:\n",
    "        report_lines.append(\"[WARN] Low samples per class. Consider:\")\n",
    "        report_lines.append(\"   - Data augmentation\")\n",
    "        report_lines.append(\"   - Regularization (dropout, weight decay)\")\n",
    "        report_lines.append(\"   - Smaller model capacity\")\n",
    "    \n",
    "    return suggested_latent, suggested_batch, suggested_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08600c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data, report_lines):\n",
    "    \"\"\"Create visualization of data quality.\"\"\"\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"5. VISUALIZATIONS\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    features = data['features_normalized']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    if features is None:\n",
    "        return\n",
    "    \n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    fig.suptitle('Data Quality Visualization', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Feature distribution (histogram of mean values per feature)\n",
    "    ax1 = axes[0, 0]\n",
    "    feature_means = np.mean(features, axis=0)\n",
    "    ax1.hist(feature_means, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('Feature Mean Value')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Distribution of Feature Means')\n",
    "    ax1.axvline(x=0, color='red', linestyle='--', label='Expected (0)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Feature variance distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    feature_stds = np.std(features, axis=0)\n",
    "    # Handle cases with low variance by using fewer bins\n",
    "    try:\n",
    "        ax2.hist(feature_stds, bins='auto', color='coral', edgecolor='black', alpha=0.7)\n",
    "    except ValueError:\n",
    "        ax2.hist(feature_stds, bins=10, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Feature Std Dev')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Distribution of Feature Standard Deviations')\n",
    "    ax2.axvline(x=1, color='red', linestyle='--', label='Expected (1)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Sample distribution (t-SNE or random 2D projection)\n",
    "    ax3 = axes[1, 0]\n",
    "    if features.shape[0] > 500:\n",
    "        # Use random projection for speed\n",
    "        np.random.seed(42)\n",
    "        idx = np.random.choice(features.shape[1], 2, replace=False)\n",
    "        proj = features[:, idx]\n",
    "    else:\n",
    "        proj = features[:, :2]\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = labels == label\n",
    "            ax3.scatter(proj[mask, 0], proj[mask, 1], c=[colors[i]], \n",
    "                       label=label, alpha=0.6, s=20)\n",
    "        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    else:\n",
    "        ax3.scatter(proj[:, 0], proj[:, 1], c='steelblue', alpha=0.5, s=20)\n",
    "    ax3.set_xlabel('Random Feature 1')\n",
    "    ax3.set_ylabel('Random Feature 2')\n",
    "    ax3.set_title('2D Random Projection of Samples')\n",
    "    \n",
    "    # 4. Class distribution bar chart\n",
    "    ax4 = axes[1, 1]\n",
    "    if labels is not None:\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        sorted_idx = np.argsort(-counts)\n",
    "        bars = ax4.bar(range(len(unique_labels)), counts[sorted_idx], \n",
    "                       color='teal', edgecolor='black', alpha=0.7)\n",
    "        ax4.set_xticks(range(len(unique_labels)))\n",
    "        ax4.set_xticklabels([unique_labels[i] for i in sorted_idx], \n",
    "                           rotation=45, ha='right', fontsize=9)\n",
    "        ax4.set_xlabel('Genre')\n",
    "        ax4.set_ylabel('Sample Count')\n",
    "        ax4.set_title('Class Distribution')\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, counts[sorted_idx]):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                    str(count), ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    viz_path = os.path.join(RESULTS_DIR, 'data_quality_visualization.png')\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    report_lines.append(f\"\\n[OK] Visualizations saved to: {viz_path}\")\n",
    "    print(f\"\\n[OK] Visualizations saved to: {viz_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb619c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "   DATA QUALITY CHECK - VAE Clustering Project\n",
      "============================================================\n",
      "\n",
      "Loading processed data...\n",
      "[OK] Loaded features_normalized.npy: shape (1959, 370), dtype float64\n",
      "[OK] Loaded features_raw.npy: shape (1959, 370), dtype float64\n",
      "[OK] Loaded labels.npy: shape (1959,), dtype <U9\n",
      "[OK] Loaded metadata.csv: 1959 rows\n",
      "[OK] Loaded config.pkl\n",
      "\n",
      "========================================\n",
      "Running quality checks...\n",
      "========================================\n",
      "\n",
      "[OK] Visualizations saved to: results\\data_quality_visualization.png\n",
      "\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Generated: 2026-01-01 11:36:19.927047\n",
      "\n",
      "============================================================\n",
      "1. DATA CONSISTENCY CHECK\n",
      "============================================================\n",
      "[OK] Features and labels have same length: 1959 samples\n",
      "[OK] Metadata and features have same length\n",
      "[OK] All data components are consistent!\n",
      "\n",
      "============================================================\n",
      "2. FEATURE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Dataset Shape: 1959 samples x 370 features\n",
      "\n",
      "--- Basic Statistics ---\n",
      "Mean: -0.008749\n",
      "Std: 1.011998\n",
      "Min: -5.351547\n",
      "Max: 7.114525\n",
      "\n",
      "--- Missing/Invalid Values ---\n",
      "[OK] No NaN values\n",
      "[OK] No Inf values\n",
      "\n",
      "--- Normalization Quality ---\n",
      "Features with mean ~ 0: 111/370 (30.0%)\n",
      "Features with std ~ 1: 370/370 (100.0%)\n",
      "[WARN] Warning: Normalization may not be optimal. Consider re-normalizing.\n",
      "\n",
      "--- Feature Correlations ---\n",
      "Highly correlated feature pairs (|r| > 0.95): 373\n",
      "[WARN] Warning: Many highly correlated features. Consider PCA dimensionality reduction.\n",
      "\n",
      "============================================================\n",
      "3. LABEL DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Number of classes: 12\n",
      "Total samples: 1959\n",
      "\n",
      "--- Class Distribution ---\n",
      "hiphop         :   260 ( 13.3%) ######\n",
      "metal          :   260 ( 13.3%) ######\n",
      "pop            :   260 ( 13.3%) ######\n",
      "rock           :   260 ( 13.3%) ######\n",
      "folk           :   160 (  8.2%) ####\n",
      "indie          :   160 (  8.2%) ####\n",
      "blues          :   100 (  5.1%) ##\n",
      "classical      :   100 (  5.1%) ##\n",
      "country        :   100 (  5.1%) ##\n",
      "disco          :   100 (  5.1%) ##\n",
      "reggae         :   100 (  5.1%) ##\n",
      "jazz           :    99 (  5.1%) ##\n",
      "\n",
      "--- Class Balance Metrics ---\n",
      "Imbalance ratio (max/min): 2.63\n",
      "[WARN] Moderate class imbalance. Consider using class weights.\n",
      "\n",
      "============================================================\n",
      "4. HYPERPARAMETER RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "--- VAE Architecture Suggestions ---\n",
      "\n",
      "Input dimension: 370\n",
      "Suggested latent dimensions: [32, 64, 128]\n",
      "Suggested encoder layers: [370, 512, 256, 128]\n",
      "Suggested decoder layers: [128, 256, 512, 370]\n",
      "\n",
      "--- Training Suggestions ---\n",
      "Dataset size: 1959 samples\n",
      "Suggested batch size: 32\n",
      "Suggested epochs: 150\n",
      "Suggested learning rate: 1e-3 to 1e-4\n",
      "Suggested beta (KL weight): 0.1 to 1.0 (start lower)\n",
      "\n",
      "--- Clustering Suggestions ---\n",
      "Number of known classes: 12\n",
      "Suggested K for K-Means: 12\n",
      "Alternative K values to try: 10 to 14\n",
      "\n",
      "--- Data-Specific Recommendations ---\n",
      "\n",
      "============================================================\n",
      "5. VISUALIZATIONS\n",
      "============================================================\n",
      "\n",
      "[OK] Visualizations saved to: results\\data_quality_visualization.png\n",
      "\n",
      "============================================================\n",
      "6. SUMMARY\n",
      "============================================================\n",
      "\n",
      "* Total samples: 1959\n",
      "* Feature dimensions: 370\n",
      "* Number of classes: 12\n",
      "* Data files present: 5/5\n",
      "\n",
      "============================================================\n",
      "END OF REPORT\n",
      "============================================================\n",
      "\n",
      "[OK] Full report saved to: results\\data_quality_report.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all quality checks.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"   DATA QUALITY CHECK - VAE Clustering Project\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    report_lines = []\n",
    "    report_lines.append(\"DATA QUALITY REPORT\")\n",
    "    report_lines.append(\"=\" * 60)\n",
    "    report_lines.append(f\"Generated: {pd.Timestamp.now()}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading processed data...\")\n",
    "    data = load_processed_data()\n",
    "    \n",
    "    # Run checks\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Running quality checks...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    check_data_consistency(data, report_lines)\n",
    "    analyze_features(data, report_lines)\n",
    "    analyze_labels(data, report_lines)\n",
    "    suggest_hyperparameters(data, report_lines)\n",
    "    visualize_data(data, report_lines)\n",
    "    \n",
    "    # Summary\n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"6. SUMMARY\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    if data['features_normalized'] is not None:\n",
    "        n_samples, n_features = data['features_normalized'].shape\n",
    "        n_classes = len(np.unique(data['labels'])) if data['labels'] is not None else \"Unknown\"\n",
    "        \n",
    "        report_lines.append(f\"\\n* Total samples: {n_samples}\")\n",
    "        report_lines.append(f\"* Feature dimensions: {n_features}\")\n",
    "        report_lines.append(f\"* Number of classes: {n_classes}\")\n",
    "        report_lines.append(f\"* Data files present: {sum(1 for v in data.values() if v is not None)}/5\")\n",
    "    \n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "    report_lines.append(\"END OF REPORT\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(RESULTS_DIR, REPORT_FILE)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    \n",
    "    # Print report to console\n",
    "    print(\"\\n\" + \"\\n\".join(report_lines))\n",
    "    print(f\"\\n[OK] Full report saved to: {report_path}\")\n",
    "    \n",
    "    return report_lines\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
