{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, \n",
    "                             davies_bouldin_score, adjusted_rand_score)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'sample_rate': 22050, 'duration': 30, 'n_mels': 128, 'n_mfcc': 40,\n",
    "    'n_fft': 2048, 'hop_length': 512, 'max_samples_per_class': 50,\n",
    "    'fixed_time_steps': 128,  # Fixed time dimension for CNN\n",
    "}\n",
    "BASE_PATH = r\"f:\\BRACU\\Semester 12 Final\\CSE425\\FInal_project\\Datasets\"\n",
    "BANGLA_PATH = os.path.join(BASE_PATH, \"Bangla_Datasets\")\n",
    "ENGLISH_PATH = os.path.join(BASE_PATH, \"English_Datasets\")\n",
    "METADATA_PATH = os.path.join(BASE_PATH, \"updated_metadata.csv\")\n",
    "OUTPUT_PATH = r\"f:\\BRACU\\Semester 12 Final\\CSE425\\FInal_project\\processed_data\"\n",
    "RESULTS_PATH = r\"f:\\BRACU\\Semester 12 Final\\CSE425\\FInal_project\\results_advanced\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "print(\"Configuration loaded!\")\n",
    "print(\"Loading metadata with lyrics...\")\n",
    "metadata_df = pd.read_csv(METADATA_PATH)\n",
    "print(f\"Metadata shape: {metadata_df.shape}\")\n",
    "print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "lyrics_dict = dict(zip(metadata_df['ID'], metadata_df['lyrics'].fillna('')))\n",
    "print(f\"Loaded {len(lyrics_dict)} lyrics entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=CONFIG['sample_rate'], duration=CONFIG['duration'])\n",
    "        expected = CONFIG['sample_rate'] * CONFIG['duration']\n",
    "        if len(audio) < expected:\n",
    "            audio = np.pad(audio, (0, expected - len(audio)))\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr):\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=CONFIG['n_mels'],\n",
    "                                          n_fft=CONFIG['n_fft'], hop_length=CONFIG['hop_length'])\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    # Resize to fixed time steps\n",
    "    if mel_db.shape[1] > CONFIG['fixed_time_steps']:\n",
    "        mel_db = mel_db[:, :CONFIG['fixed_time_steps']]\n",
    "    else:\n",
    "        mel_db = np.pad(mel_db, ((0, 0), (0, CONFIG['fixed_time_steps'] - mel_db.shape[1])))\n",
    "    return mel_db\n",
    "\n",
    "def extract_mfcc(audio, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=CONFIG['n_mfcc'],\n",
    "                                 n_fft=CONFIG['n_fft'], hop_length=CONFIG['hop_length'])\n",
    "    if mfcc.shape[1] > CONFIG['fixed_time_steps']:\n",
    "        mfcc = mfcc[:, :CONFIG['fixed_time_steps']]\n",
    "    else:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, CONFIG['fixed_time_steps'] - mfcc.shape[1])))\n",
    "    return mfcc\n",
    "\n",
    "print(\"Audio extraction functions defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: Lyrics Embedding Function\n",
    "# ============================================================================\n",
    "def create_lyrics_embeddings(lyrics_list, max_features=100):\n",
    "    \"\"\"Create TF-IDF embeddings for lyrics.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    # Handle empty lyrics\n",
    "    lyrics_cleaned = [l if l and len(str(l)) > 0 else ' ' for l in lyrics_list]\n",
    "    embeddings = vectorizer.fit_transform(lyrics_cleaned).toarray()\n",
    "    return embeddings, vectorizer\n",
    "\n",
    "print(\"Lyrics embedding function defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: Collect and Process Audio Files\n",
    "# ============================================================================\n",
    "def collect_audio_files():\n",
    "    audio_files = []\n",
    "    for path, lang in [(BANGLA_PATH, 'bn'), (ENGLISH_PATH, 'en')]:\n",
    "        if os.path.exists(path):\n",
    "            for genre in os.listdir(path):\n",
    "                genre_path = os.path.join(path, genre)\n",
    "                if os.path.isdir(genre_path):\n",
    "                    files = [f for f in os.listdir(genre_path) if f.endswith('.wav')][:CONFIG['max_samples_per_class']]\n",
    "                    for f in files:\n",
    "                        file_id = os.path.splitext(f)[0]\n",
    "                        audio_files.append({\n",
    "                            'path': os.path.join(genre_path, f),\n",
    "                            'language': lang, 'genre': genre,\n",
    "                            'filename': f, 'id': file_id,\n",
    "                            'lyrics': lyrics_dict.get(file_id, '')\n",
    "                        })\n",
    "    return audio_files\n",
    "\n",
    "audio_files = collect_audio_files()\n",
    "print(f\"Collected {len(audio_files)} audio files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting features...\")\n",
    "mel_spectrograms, mfccs, labels, lyrics_list, file_metadata = [], [], [], [], []\n",
    "\n",
    "for file_info in tqdm(audio_files, desc=\"Processing\"):\n",
    "    audio, sr = load_audio(file_info['path'])\n",
    "    if audio is not None:\n",
    "        try:\n",
    "            mel_spectrograms.append(extract_mel_spectrogram(audio, sr))\n",
    "            mfccs.append(extract_mfcc(audio, sr))\n",
    "            labels.append(file_info['genre'])\n",
    "            lyrics_list.append(file_info['lyrics'])\n",
    "            file_metadata.append(file_info)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "mel_spectrograms = np.array(mel_spectrograms)\n",
    "mfccs = np.array(mfccs)\n",
    "labels = np.array(labels)\n",
    "print(f\"\\nMel spectrograms shape: {mel_spectrograms.shape}\")\n",
    "print(f\"MFCCs shape: {mfccs.shape}\")\n",
    "\n",
    "# Create lyrics embeddings\n",
    "lyrics_embeddings, tfidf_vectorizer = create_lyrics_embeddings(lyrics_list)\n",
    "print(f\"Lyrics embeddings shape: {lyrics_embeddings.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1442e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_height=128, input_width=128, latent_dim=64):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flat_size = 256 * (input_height // 16) * (input_width // 16)\n",
    "        self.fc_mu = nn.Linear(self.flat_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flat_size, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flat_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, input_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "        self.h_out = input_height // 16\n",
    "        self.w_out = input_width // 16\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x).view(-1, self.flat_size)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu + torch.randn_like(std) * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(-1, 256, self.h_out, self.w_out)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "\n",
    "    def get_latent(self, x):\n",
    "        mu, _ = self.encode(x)\n",
    "        return mu\n",
    "\n",
    "print(\"Convolutional VAE defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridVAE(nn.Module):\n",
    "    def __init__(self, audio_latent_dim=32, lyrics_dim=100, combined_latent_dim=48):\n",
    "        super(HybridVAE, self).__init__()\n",
    "        self.combined_latent_dim = combined_latent_dim\n",
    "        \n",
    "        # Audio encoder (from ConvVAE)\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "        )\n",
    "        self.audio_flat = 128 * 16 * 16\n",
    "        self.audio_fc = nn.Linear(self.audio_flat, audio_latent_dim)\n",
    "        \n",
    "        # Lyrics encoder\n",
    "        self.lyrics_encoder = nn.Sequential(\n",
    "            nn.Linear(lyrics_dim, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, audio_latent_dim), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined latent space\n",
    "        combined_input = audio_latent_dim * 2\n",
    "        self.fc_mu = nn.Linear(combined_input, combined_latent_dim)\n",
    "        self.fc_logvar = nn.Linear(combined_input, combined_latent_dim)\n",
    "        \n",
    "        # Decoder (audio only)\n",
    "        self.fc_decode = nn.Linear(combined_latent_dim, self.audio_flat)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def encode(self, audio, lyrics):\n",
    "        audio_h = self.audio_encoder(audio).view(-1, self.audio_flat)\n",
    "        audio_feat = self.audio_fc(audio_h)\n",
    "        lyrics_feat = self.lyrics_encoder(lyrics)\n",
    "        combined = torch.cat([audio_feat, lyrics_feat], dim=1)\n",
    "        return self.fc_mu(combined), self.fc_logvar(combined)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu + torch.randn_like(std) * std\n",
    "\n",
    "    def forward(self, audio, lyrics):\n",
    "        mu, logvar = self.encode(audio, lyrics)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        h = self.fc_decode(z).view(-1, 128, 16, 16)\n",
    "        recon = self.decoder(h)\n",
    "        return recon, mu, logvar, z\n",
    "\n",
    "    def get_latent(self, audio, lyrics):\n",
    "        mu, _ = self.encode(audio, lyrics)\n",
    "        return mu\n",
    "\n",
    "print(\"Hybrid VAE defined!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
